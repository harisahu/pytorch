{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713b21f8",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5157e5",
   "metadata": {},
   "source": [
    "* Create a Neural Network with One Hidden Layer using nn.Module.\n",
    "* Create a Neural Network with One Hidden Layer using nn.Sequential.\n",
    "* Train the Neural Network model.\n",
    "* Explain how adding more neurons in the hidden layer can improve a model.\n",
    "* Construct Networks with Multiple Dimensional input in PyTorch.\n",
    "* Explain what Overfitting and Underfitting are.\n",
    "* Implement Multi-Class Neural Networks in PyTorch.\n",
    "* Describe what back propagation and the vanishing gradient is.\n",
    "* Implement Sigmoid, Tanh and Relu activation functions in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba811ce",
   "metadata": {},
   "source": [
    "## What is a neural network?\n",
    "A neural network is a function that can be used to approximate most functions using a set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee5a4d",
   "metadata": {},
   "source": [
    "## Single Layer Neural Network to classify non-linearly separable 1D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#manually seed the random num generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "#function for plotting the model\n",
    "def plotStuff(X,Y, model, epoch, leg=True):\n",
    "        plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch'+str(epoch)))\n",
    "        plt.plot(X.numpy(), Y.numpy(), 'r')\n",
    "        plt.xlabel('X')\n",
    "        if leg == True:\n",
    "            plt.legend()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "#define the model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1= nn.Linear(D_in,H)\n",
    "        self.linear2= nn.Linear(H,D_out)\n",
    "        #to facilitate visualization and inspection of network during traning\n",
    "        self.a1= None\n",
    "        self.l1= None\n",
    "        self.l2= None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.l1= self.linear1(x)\n",
    "        self.a1= sigmoid(self.l1)\n",
    "        self.l2= self.linear2(self.a1)\n",
    "        yhat= sigmoid(self.l2)\n",
    "        return yhat\n",
    "\n",
    "\n",
    "#define training function\n",
    "def train(Y, X, model, optimizer, criterion, epochs=1000):\n",
    "    cost= []\n",
    "    for epoch in range(epochs):\n",
    "        total=0\n",
    "        for y,x in zip(Y,X):\n",
    "            yhat= model(x)\n",
    "            loss= criterion(yhat,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            #cumulitive loss\n",
    "            total+= loss.item()\n",
    "        cost.append(total)\n",
    "        \n",
    "        #visualization and model inspections at certain intervals\n",
    "        \"\"\"\n",
    "        if epoch%300 == 0:\n",
    "            plotStuff(X,Y, model, epoch, leg=True)\n",
    "            plt.show()\n",
    "            model(X)\n",
    "            #The detach() method applied on self.a1 creates a new tensor that \n",
    "            #is detached from the computational graph, ensuring that \n",
    "            #it won't be part of any further computation or gradient calculation.\n",
    "            plt.scatter(model.a1.detach().numpy()[:,0], model.a1.detach().numpy()[:,1], c=Y.numpy().reshape(-1))\n",
    "            plt.title('activations')\n",
    "            plt.show()\n",
    "        \"\"\"\n",
    "    return cost\n",
    "    \n",
    "\n",
    "#Make some data\n",
    "X= torch.arange(-20,20,1).view(-1,1).type(torch.FloatTensor)\n",
    "Y= torch.zeros(X.shape[0])\n",
    "Y[(X[:,0] > -4) & (X[:,0] < 4)]= 1.0\n",
    "\n",
    "\n",
    "#criterion\n",
    "def criterion_cross(outputs, labels):\n",
    "    out= -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "#size of input\n",
    "D_in= 1\n",
    "#size of hidden layer\n",
    "H= 2\n",
    "#number of outputs\n",
    "D_out=1\n",
    "#learning rate\n",
    "learning_rate = 0.1\n",
    "#create the model\n",
    "model= Net(D_in, H, D_out)\n",
    "#optimizer\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "\n",
    "\n",
    "#train the model\n",
    "cost_cross= train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n",
    "\n",
    "#plot the loss\n",
    "plt.plot(cost_cross)\n",
    "plt.xlabel('epoch')\n",
    "plt.title('cross entropy loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede1cc0",
   "metadata": {},
   "source": [
    "## Neural Network with one hidden layer with two neurons using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd9496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a982551",
   "metadata": {},
   "source": [
    "## Backpropogation\n",
    "It reduces the number of computation that is require to calculate gradients. backpropogation uses the graidents of the output layer to calculate the graidents of the hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19051436",
   "metadata": {},
   "source": [
    "## problem of Using Sigmoid function in deep neural networks\n",
    "It's vanishing graident problem. the value of the derivative of the sigmoid function becomes very small for the deeper hidden layers. \n",
    "To solve this problem change the activation function or use optimization techniques to reduce this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c417752",
   "metadata": {},
   "source": [
    "## Commanly Used Activation Function in neural networks\n",
    "\n",
    "* ReLu performs better as compared to tanh and sigmoid function if we had more hidden layers.\n",
    "\n",
    "* what is the problem with tanh and sigmoid activation function?\n",
    "  The derivative is near zero in many regions. this causes vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5101d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
