{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f3560c",
   "metadata": {},
   "source": [
    "##  In the realm of neural networks, how do you perceive the delicate balance between model complexity, overfitting, and underfitting? Furthermore, could you delve into the strategy you might employ to strike this balance effectively, particularly when confronted with resource limitations?\n",
    "\n",
    "if our model is too complicated, we risk overfitting. if our model is overly simplified, we risk underfitting. we could try different combinations of the number of neurons and layers, but it will take too much time and resources.\n",
    "<br><br>\n",
    "A solution is to start with a model that is complex and apply a form of regularization called dropout. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5dff9",
   "metadata": {},
   "source": [
    "## What is dropout\n",
    "\n",
    "* In neural networks, dropout is a regularization technique used to prevent overfitting. It involves randomly \"dropping out\" (i.e., setting to zero) a fraction of the neurons during each training iteration. This helps prevent the network from relying too heavily on any individual neuron, forcing it to learn more robust and generalized features.\n",
    "\n",
    "* Also, as there are more layers and more neurons in out network, the chances of overfitting increases, so it also helps to decrease the problem of overfitting.\n",
    "\n",
    "* Also, it indirectly introduces noise in our data, so that model during training, the model generalizes well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d4d75",
   "metadata": {},
   "source": [
    "## Why during training, if we apply droputs, then the output of the neurons which are not dropped are normalized by 1/(1-p)\n",
    "\n",
    "* `The reason for normalizing the output of active neurons during dropout training is to ensure that the expected value of the output remains the same as in the absence of dropout.` \n",
    "<br>\n",
    "This scaling ensures that the neuron's expected output remains the same, which helps with the training process and maintaining the network's behavior as close to the original network as possible when dropout is not applied.\n",
    "<br>\n",
    "Also, during inference or testing (when dropout is not active), the scaling is typically not applied, and the full strength of the neurons is used for prediction.\n",
    "\n",
    "\n",
    "* When dropout is applied to a neuron with probability 'p', the neuron is retained with probability '1 - p', and it is dropped out with probability 'p'.\n",
    "Consider the following scenario: Without dropout, a neuron's output is 'x'.                                                     With dropout, the neuron's output will be 'x' with probability '1 - p', and '0' with probability 'p'. To maintain the expected value of the output, we need to scale the retained output 'x' by '1 / (1 - p)'.\n",
    "\n",
    "    Mathematically, let's denote the output of the neuron without dropout as 'x', and the output with dropout as 'y'. \n",
    "    Then:\n",
    "    <br>\n",
    "    E[y] = (1 - p) * x + p * 0\n",
    "         = (1 - p) * x\n",
    "\n",
    "    To make the expected value of 'y' the same as 'x', we need to scale 'y' by '1 / (1 - p)':\n",
    "\n",
    "    E[scaled_y] = (1 / (1 - p)) * E[y]\n",
    "                = (1 / (1 - p)) * (1 - p) * x\n",
    "                = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5fa9c2",
   "metadata": {},
   "source": [
    "## Why during inference, the output of all the neurons are scaled by (1-p)\n",
    "\n",
    "During inference or testing, dropout is turned off, and there is no scaling or zeroing out of neuron outputs. The full network is used for making predictions. so, each neuron is retained with probability (1-p)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989701e0",
   "metadata": {},
   "source": [
    "## How to decided the value of hyper parameter p used in this technique?\n",
    "\n",
    "* it's important to note that the value of 'p' is not fixed and can be adjusted based on the specific problem and architecture. \n",
    "\n",
    "* `Smaller values of 'p' (e.g., 0.2 or 0.3) might be appropriate for more complex models or datasets with larger amounts of noise. On the other hand, larger values of 'p' might be used for simpler architectures or when dealing with less noisy data.`\n",
    "\n",
    "* In practice, the choice of 'p' can be treated as a hyperparameter that you tune through experimentation to find the value that best balances regularization and model performance for your specific task. It's often recommended to perform a grid search or use techniques like random search to find an optimal value for 'p' alongside other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc020d",
   "metadata": {},
   "source": [
    "## Classification with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ade526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need for this lab\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# The function for plotting the diagram\n",
    "def plot_decision_regions_3class(data_set, model=None):\n",
    "    cmap_light = ListedColormap([ '#0000FF','#FF0000'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n",
    "    X = data_set.x.numpy()\n",
    "    y = data_set.y.numpy()\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 \n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    newdata = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    Z = data_set.multi_dim_poly(newdata).flatten()\n",
    "    f = np.zeros(Z.shape)\n",
    "    f[Z > 0] = 1\n",
    "    f = f.reshape(xx.shape)\n",
    "    if model != None:\n",
    "        model.eval()\n",
    "        XX = torch.Tensor(newdata)\n",
    "        _, yhat = torch.max(model(XX), 1)\n",
    "        yhat = yhat.numpy().reshape(xx.shape)\n",
    "        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n",
    "        plt.contour(xx, yy, f, cmap=plt.cm.Paired)\n",
    "    else:\n",
    "        plt.contour(xx, yy, f, cmap=plt.cm.Paired)\n",
    "        plt.pcolormesh(xx, yy, f, cmap=cmap_light) \n",
    "\n",
    "    plt.title(\"decision region vs True decision boundary\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# The function for calculating accuracy\n",
    "def accuracy(model, data_set):\n",
    "    _, yhat = torch.max(model(data_set.x), 1)\n",
    "    return (yhat == data_set.y).numpy().mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create data class for creating dataset object\n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):\n",
    "        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T\n",
    "        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))\n",
    "        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()\n",
    "        self.a = a\n",
    "       \n",
    "        self.y = np.zeros(N_SAMPLES)\n",
    "        self.y[self.f > 0] = 1\n",
    "        self.y = torch.from_numpy(self.y).type(torch.LongTensor)\n",
    "        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)\n",
    "        self.x = self.x + noise_std * torch.randn(self.x.size())\n",
    "        self.f = torch.from_numpy(self.f)\n",
    "        self.a = a\n",
    "        if train == True:\n",
    "            torch.manual_seed(1)\n",
    "            self.x = self.x + noise_std * torch.randn(self.x.size())\n",
    "            torch.manual_seed(0)\n",
    "        \n",
    "    # Getter        \n",
    "    def __getitem__(self, index):    \n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Plot the diagram\n",
    "    def plot(self):\n",
    "        X = data_set.x.numpy()\n",
    "        y = data_set.y.numpy()\n",
    "        h = .02\n",
    "        x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "        y_min, y_max = X[:, 1].min(), X[:, 1].max() \n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()\n",
    "        f = np.zeros(Z.shape)\n",
    "        f[Z > 0] = 1\n",
    "        f = f.reshape(xx.shape)\n",
    "        \n",
    "        plt.title('True decision boundary  and sample points with noise ')\n",
    "        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') \n",
    "        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')\n",
    "        plt.contour(xx, yy, f,cmap=plt.cm.Paired)\n",
    "        plt.xlim(0,1)\n",
    "        plt.ylim(0,1)\n",
    "        plt.legend()\n",
    "    \n",
    "    # Make a multidimension ploynomial function\n",
    "    def multi_dim_poly(self, x):\n",
    "        x = np.matrix(x)\n",
    "        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])\n",
    "        out = np.array(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object\n",
    "data_set = Data(noise_std=0.2)\n",
    "data_set.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some validation data\n",
    "torch.manual_seed(0) \n",
    "validation_set = Data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a286f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Net Class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, in_size, n_hidden, out_size, p=0):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p=p)\n",
    "        self.linear1 = nn.Linear(in_size, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = nn.Linear(n_hidden, out_size)\n",
    "    \n",
    "    # Prediction function\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop(self.linear1(x)))\n",
    "        x = F.relu(self.drop(self.linear2(x)))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Create two model objects: model without dropout and model with dropout\n",
    "model = Net(2, 300, 2)\n",
    "model_drop = Net(2, 300, 2, p=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode\n",
    "model_drop.train()\n",
    "\n",
    "# Set optimizer functions and criterion functions\n",
    "optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the LOSS dictionary to store the loss\n",
    "LOSS = {}\n",
    "LOSS['training data no dropout'] = []\n",
    "LOSS['validation data no dropout'] = []\n",
    "LOSS['training data dropout'] = []\n",
    "LOSS['validation data dropout'] = []\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "epochs = 500\n",
    "def train_model(epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #all the samples are used for training \n",
    "        yhat = model(data_set.x)\n",
    "        yhat_drop = model_drop(data_set.x)\n",
    "        loss = criterion(yhat, data_set.y)\n",
    "        loss_drop = criterion(yhat_drop, data_set.y)\n",
    "\n",
    "        #store the loss for both the training and validation data for both models \n",
    "        LOSS['training data no dropout'].append(loss.item())\n",
    "        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())\n",
    "        LOSS['training data dropout'].append(loss_drop.item())\n",
    "        model_drop.eval()\n",
    "        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())\n",
    "        model_drop.train()\n",
    "\n",
    "        optimizer_ofit.zero_grad()\n",
    "        optimizer_drop.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_drop.backward()\n",
    "        optimizer_ofit.step()\n",
    "        optimizer_drop.step()\n",
    "        \n",
    "train_model(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation model\n",
    "model_drop.eval()\n",
    "\n",
    "# Print out the accuracy of the model without dropout\n",
    "print(\"The accuracy of the model without dropout: \", accuracy(model, validation_set))\n",
    "\n",
    "# Print out the accuracy of the model with dropout\n",
    "print(\"The accuracy of the model with dropout: \", accuracy(model_drop, validation_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2438037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary and the prediction\n",
    "plot_decision_regions_3class(data_set)\n",
    "\n",
    "# The model without dropout\n",
    "plot_decision_regions_3class(data_set, model)\n",
    "\n",
    "# The model with dropout\n",
    "plot_decision_regions_3class(data_set, model_drop)\n",
    "\n",
    "# Plot the LOSS\n",
    "plt.figure(figsize=(6.1, 10))\n",
    "def plot_LOSS():\n",
    "    for key, value in LOSS.items():\n",
    "        plt.plot(np.log(np.array(value)), label=key)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"Log of cost or total loss\")\n",
    "\n",
    "plot_LOSS()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
